@misc{schulman2017proximal,
	abstract = {We propose a new family of policy gradient methods for
	            reinforcement learning, which alternate between sampling data
	            through interaction with the environment, and optimizing a "
	            surrogate" objective function using stochastic gradient ascent.
	            Whereas standard policy gradient methods perform one gradient
	            update per data sample, we propose a novel objective function that
	            enables multiple epochs of minibatch updates. The new methods,
	            which we call proximal policy optimization (PPO), have some of the
	            benefits of trust region policy optimization (TRPO), but they are
	            much simpler to implement, more general, and have better sample
	            complexity (empirically). Our experiments test PPO on a collection
	            of benchmark tasks, including simulated robotic locomotion and
	            Atari game playing, and we show that PPO outperforms other online
	            policy gradient methods, and overall strikes a favorable balance
	            between sample complexity, simplicity, and wall-time.},
	added-at = {2023-08-14T19:18:46.000+0200},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford,
	          Alec and Klimov, Oleg},
	biburl = {
	          https://www.bibsonomy.org/bibtex/2ebf49efaa2caed1307b2ccb7529eb9d6/vincentqb
	          },
	description = {Proximal Policy Optimization Algorithms},
	interhash = {f57ff463a90dbafb77d55a25aea8355c},
	intrahash = {ebf49efaa2caed1307b2ccb7529eb9d6},
	keywords = {ppo},
	timestamp = {2023-08-14T19:18:46.000+0200},
	title = {{Proximal Policy Optimization Algorithms}},
	url = {http://arxiv.org/abs/1707.06347},
	x-fetchedfrom = {Bibsonomy},
	year = 2017,
}

@inproceedings{uradnik2024reducing,
	author = {\'{U} and Sychrovsk\'{y}, David and Čern\'{y}, Jakub and Čern\'{y},
	          Martin},
	title = {Reducing Optimism Bias in Incomplete Cooperative Games},
	year = {2024},
	isbn = {9798400704864},
	publisher = {International Foundation for Autonomous Agents and Multiagent
	             Systems},
	address = {Richland, SC},
	abstract = {Cooperative game theory has diverse applications in contemporary
	            artificial intelligence, including domains like interpretable
	            machine learning, resource allocation, and collaborative
	            decision-making. However, specifying a cooperative game entails
	            assigning values to exponentially many coalitions, and obtaining
	            even a single value can be resource-intensive in practice. Yet
	            simply leaving certain coalition values undisclosed introduces
	            ambiguity regarding individual contributions to the collective
	            grand coalition. This ambiguity often leads to players holding
	            overly optimistic expectations, stemming from either inherent
	            biases or strategic considerations, frequently resulting in
	            collective claims exceeding the actual grand coalition value. In
	            this paper, we present a framework aimed at optimizing the sequence
	            for revealing coalition values, with the overarching goal of
	            efficiently closing the gap between players' expectations and
	            achievable outcomes in cooperative games. Our contributions are
	            threefold: (i) we study the individual players' optimistic
	            completions of games with missing coalition values along with the
	            arising gap, and investigate its analytical characteristics that
	            facilitate more efficient optimization; (ii) we develop methods to
	            minimize this gap over classes of games with a known prior by
	            disclosing values of additional coalitions in both offline and
	            online fashion; and (iii) we empirically demonstrate the
	            algorithms' performance in practical scenarios, together with an
	            investigation into the typical order of revealing coalition values.
	            },
	booktitle = {Proceedings of the 23rd International Conference on Autonomous
	             Agents and Multiagent Systems},
	pages = {1847–1855},
	numpages = {9},
	keywords = {active learning, incomplete cooperative games, shapley value,
	            superadditive set functions, value querying},
	series = {AAMAS '24},
}

@book{sutton2018reinforcement,
	author = {Sutton, Richard S. and Barto, Andrew G.},
	publisher = {MIT press},
	title = {{Reinforcement learning: An introduction}},
	x-fetchedfrom = {Google Scholar},
	year = {2018},
}

@article{9252865,
	author = {Beliakov, Gleb},
	journal = {IEEE Transactions on Fuzzy Systems},
	title = {On Random Generation of Supermodular Capacities},
	year = {2022},
	volume = {30},
	number = {1},
	pages = {293-296},
	keywords = {Games;Markov processes;Analytical models;Game theory;Task
	            analysis;Tools;Indexes;Capacity;fuzzy measure;multiple criteria
	            decision making;random sampling;supermodularity},
	doi = {10.1109/TFUZZ.2020.3036699},
}


